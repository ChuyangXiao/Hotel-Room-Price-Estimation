{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "df = pd.read_csv('Data/df_final.csv',index_col = 0)\n",
    "\n",
    "# drop the columns that includes only one value\n",
    "# and categorical variables that has too many values\n",
    "df = df.drop(columns = ['israteperstay'])\n",
    "\n",
    "# convert to some binary features to bool\n",
    "df['ispromo']  = (df['ispromo']== 'Y')\n",
    "\n",
    "df['Source']  = (df['Source']== 5)\n",
    "\n",
    "# Use target encoding to transform the categorical variables that have too many unique values\n",
    "encoder = TargetEncoder()\n",
    "df[['roomtype','city','country','ratetype','propertytype']] = encoder.fit_transform(df[['roomtype',\n",
    "                                                                                        'city',\n",
    "                                                                                        'country',\n",
    "                                                                                        'ratetype',\n",
    "                                                                                        'propertytype']], \n",
    "                                                                                    df['price'])\n",
    "\n",
    "# convert the boolean terms into integers\n",
    "df = df*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the feature and target from the dataset\n",
    "X_dev = df.loc[:, df.columns != 'price']\n",
    "y_dev = df.loc[:, df.columns == 'price']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, \n",
    "                                                    y_dev, \n",
    "                                                    test_size=0.2,  \n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45715842",
   "metadata": {},
   "source": [
    "## Baseline Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline regressor to examine advanced models\n",
    "dummy_regr = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "\n",
    "train_mae_dummy = sm.mean_absolute_error(y_train, \n",
    "                                   dummy_regr.predict(X_train))\n",
    "\n",
    "test_mae_dummy = sm.mean_absolute_error(y_test, \n",
    "                                  dummy_regr.predict(X_test))\n",
    "\n",
    "print(f\"The best training MAE is   : {round(train_mae_dummy,3)}\")\n",
    "print(f\"The best testing MAE is    : {round(test_mae_dummy,3)}\")\n",
    "\n",
    "print(f\"The training accuracy is   : {round(dummy_regr.score(X_train, y_train)*100,4)}%\")\n",
    "print(f\"The training accuracy is   : {round(dummy_regr.score(X_test, y_test)*100,4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a8035",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "### Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9be015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_para = {'max_depth':[25,50,75,100],\n",
    "#              'max_features':[15,20,25,30]}\n",
    "\n",
    "# clf_dt = GridSearchCV(DecisionTreeRegressor(random_state = 123), \n",
    "#                    tree_para, \n",
    "#                    cv=5)\n",
    "\n",
    "# clf_dt.fit(X_train, y_train)\n",
    "\n",
    "# train_mae_dt = sm.mean_absolute_error(y_train, \n",
    "#                                    clf_dt.best_estimator_.predict(X_train))\n",
    "\n",
    "# test_mae_dt = sm.mean_absolute_error(y_test, \n",
    "#                                   clf_dt.best_estimator_.predict(X_test))\n",
    "\n",
    "# print(f\"The optimal parameter is   : {clf_dt.best_params_}\")\n",
    "# print(f\"The best training MAE is   : {round(train_mae_dt,3)}\")\n",
    "# print(f\"The best testing MAE is    : {round(test_mae_dt,3)}\")\n",
    "\n",
    "# print(f\"The training accuracy is   : {round(clf_dt.score(X_train, y_train)*100,4)}%\")\n",
    "# print(f\"The testing accuracy is    : {round(clf_dt.score(X_test, y_test)*100,4)}%\")\n",
    "\n",
    "# The optimal parameter is   : {'max_depth': 25, 'max_features': 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state = 123, \n",
    "                           max_depth= 25, \n",
    "                           max_features= 15)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "train_mae_dt = sm.mean_absolute_error(y_train, \n",
    "                                      dt.best_estimator_.predict(X_train))\n",
    "\n",
    "test_mae_dt = sm.mean_absolute_error(y_test, \n",
    "                                     dt.best_estimator_.predict(X_test))\n",
    "\n",
    "print(f\"The best training MAE is   : {round(train_mae_dt,3)}\")\n",
    "print(f\"The best testing MAE is    : {round(test_mae_dt,3)}\")\n",
    "\n",
    "print(f\"The training accuracy is   : {round(dt.score(X_train, y_train)*100,4)}%\")\n",
    "print(f\"The testing accuracy is    : {round(dt.score(X_test, y_test)*100,4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dedd12",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Consisting with the results above, we could find that there is an obvious overfitting existing in the model above (the training accuracy greatly exceeds the testing accuracy). Thus, some techniques will be implemented to avoid the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf_dt.best_estimator_.feature_importances_\n",
    "\n",
    "k = 5\n",
    "\n",
    "idx = np.argpartition(feature_importance, k)\n",
    "\n",
    "# drop the 5 least important features\n",
    "X_train_simplified = X_train.drop(columns = X_train.columns[idx[:k]])\n",
    "\n",
    "X_test_simplified = X_test.drop(columns = X_train.columns[idx[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_para = {'max_depth':[25,50,75,100],\n",
    "#              'max_features':[15,20,25,30]}\n",
    "\n",
    "# clf_dt_simp = GridSearchCV(DecisionTreeRegressor(random_state = 123), \n",
    "#                    tree_para, \n",
    "#                    cv=5)\n",
    "\n",
    "# clf_dt_simp.fit(X_train_simplified, y_train)\n",
    "\n",
    "# train_mae_dt_simp = sm.mean_absolute_error(y_train, \n",
    "#                                    clf_dt_simp.best_estimator_.predict(X_train_simplified))\n",
    "\n",
    "# test_mae_dt_simp = sm.mean_absolute_error(y_test, \n",
    "#                                   clf_dt_simp.best_estimator_.predict(X_test_simplified))\n",
    "\n",
    "# print(f\"The optimal parameter is   : {clf_dt_simp.best_params_}\")\n",
    "# print(f\"The best training MAE is : {round(train_mae_dt_simp,3)}\")\n",
    "# print(f\"The best testing MAE is  : {round(test_mae_dt_simp,3)}\")\n",
    "\n",
    "# print(f\"The training accuracy is   : {round(clf_dt_simp.score(X_train_simplified , y_train)*100,4)}%\")\n",
    "# print(f\"The testing accuracy is    : {round(clf_dt_simp.score(X_test_simplified, y_test)*100,4)}%\")\n",
    "\n",
    "# The optimal parameter is   : {'bootstrap': False, 'max_depth': 50, 'max_features': 'log2', 'min_samples_split': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_simp = DecisionTreeRegressor(random_state = 123, \n",
    "                                max_depth = 25, \n",
    "                                max_features= 15)\n",
    "\n",
    "dt_simp.fit(X_train, y_train)\n",
    "\n",
    "train_mae_dt_simp = sm.mean_absolute_error(y_train, \n",
    "                                      dt.best_estimator_.predict(X_train_simp))\n",
    "\n",
    "test_mae_dt_simp = sm.mean_absolute_error(y_test, \n",
    "                                     dt.best_estimator_.predict(X_test_simp))\n",
    "\n",
    "print(f\"The best training MAE is   : {round(train_mae_dt_simp,3)}\")\n",
    "print(f\"The best testing MAE is    : {round(test_mae_dt_simp,3)}\")\n",
    "\n",
    "print(f\"The training accuracy is   : {round(dt.score(X_train_simp, y_train)*100,4)}%\")\n",
    "print(f\"The testing accuracy is    : {round(dt.score(X_test_simp, y_test)*100,4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc759b",
   "metadata": {},
   "source": [
    "Thus, we could see that simply the feature selection could not solve the issue of overfitting. Thus, another accessible method to reduce the overfitting is to use an ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce0bb4",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15adaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_para = {'max_depth':[25,50,75,100,125,150],\n",
    "#              'max_features':['auto', 'sqrt', 'log2'],\n",
    "#              'min_samples_split': [2, 5, 10],\n",
    "#              'bootstrap':[True, False]\n",
    "#              }\n",
    "\n",
    "# clf_rf = GridSearchCV(RandomForestRegressor(random_state = 123), \n",
    "#                    tree_para, \n",
    "#                    cv=5)\n",
    "\n",
    "# clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# train_mae_rf = sm.mean_absolute_error(y_train, \n",
    "#                                       clf_rf.best_estimator_.predict(X_train))\n",
    "\n",
    "# test_mae_rf = sm.mean_absolute_error(y_test, \n",
    "#                                      clf_rf.best_estimator_.predict(X_test))\n",
    "\n",
    "# print(f\"The optimal parameter is   : {clf_rf.best_params_}\")\n",
    "# print(f\"The best training MAE is   : {round(train_mae_rf,3)}\")\n",
    "# print(f\"The best testing MAE is    : {round(test_mae_rf,3)}\")\n",
    "\n",
    "# print(f\"The training accuracy is   : {round(clf_rf.score(X_train_simplified, y_train)*100,4)}%\")\n",
    "# print(f\"The testing accuracy is    : {round(clf_rf.score(X_test_simplified, y_test)*100,4)}%\")\n",
    "\n",
    "# The optimal parameter is   : {'bootstrap': False, 'max_depth': 50, 'max_features': 'log2', 'min_samples_split': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the complete features to train the random forest model\n",
    "rf = RandomForestRegressor(random_state = 123, \n",
    "                           bootstrap = False, \n",
    "                           max_depth = 50, \n",
    "                           max_features = 'log2',\n",
    "                           min_samples_split = 5)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_mae_rf = sm.mean_absolute_error(y_train, \n",
    "                                      rf.predict(X_train))\n",
    "\n",
    "test_mae_rf = sm.mean_absolute_error(y_test, \n",
    "                                     rf.predict(X_test))\n",
    "\n",
    "print(f\"The best training MAE is   : {round(train_mae_rf,3)}\")\n",
    "print(f\"The best testing MAE is    : {round(test_mae_rf,3)}\")\n",
    "\n",
    "print(f\"The training accuracy is   : {round(rf.score(X_train, y_train)*100,4)}%\")\n",
    "print(f\"The testing accuracy is    : {round(rf.score(X_test, y_test)*100,4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99662694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75c106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf691b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb5fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_para = {'n_estimators':[25,50,75,100],\n",
    "             'max_depth':[25,50,75,100],\n",
    "             }\n",
    "\n",
    "clf_gbr = GridSearchCV(GradientBoostingRegressor(random_state = 123), \n",
    "                   tree_para, \n",
    "                   cv=5)\n",
    "\n",
    "clf_gbr.fit(X_train, y_train)\n",
    "\n",
    "train_mae_gbr = sm.mean_absolute_error(y_train, \n",
    "                                   clf_gbr.best_estimator_.predict(X_train_simplified))\n",
    "\n",
    "test_mae_gbr = sm.mean_absolute_error(y_test, \n",
    "                                  clf_gbr.best_estimator_.predict(X_test_simplified))\n",
    "\n",
    "print(f\"The optimal parameter is   : {clf_gbr.best_params_}\")\n",
    "print(f\"The best training MAE is : {round(train_mae_gbr,3)}\")\n",
    "print(f\"The best testing MAE is  : {round(test_mae_gbr,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10bdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_para = {'n_estimators':[25,50,75,100], \n",
    "             'max_depth':[25,50,75,100], \n",
    "             'eta':[0.01, 0.05]\n",
    "             }\n",
    "\n",
    "clf_xgb = GridSearchCV(XGBRegressor(random_state = 123,\n",
    "                                verbosity = 0), \n",
    "                   tree_para, \n",
    "                   cv=5)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "train_mae_xgb = sm.mean_absolute_error(y_train, \n",
    "                                   clf_xgb.best_estimator_.predict(X_train))\n",
    "\n",
    "test_mae_xgb = sm.mean_absolute_error(y_test, \n",
    "                                  clf_xgb.best_estimator_.predict(X_test))\n",
    "\n",
    "print(f\"The optimal parameter is   : {clf_xgb.best_params_}\")\n",
    "print(f\"The best training MAE is : {round(train_mae_xgb,3)}\")\n",
    "print(f\"The best testing MAE is  : {round(test_mae_xgb,3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4937f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79ac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6725f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e012df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac625c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e908933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0ee29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080ac41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f96902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
